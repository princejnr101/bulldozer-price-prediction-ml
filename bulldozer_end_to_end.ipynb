{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a487eab",
   "metadata": {},
   "source": [
    "# Bluebook for Bulldozers â€” End-to-End Regression (RMSLE)\n",
    "\n",
    "Run this notebook **top-to-bottom**. It is written to prevent:\n",
    "- *File not found* (auto-detects CSV location/name)\n",
    "- *DTypePromotionError* (drops datetime after extracting date parts)\n",
    "- *KeyError on dropped columns* (recomputes columns + rebuilds pipeline after transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c444d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error, r2_score, make_scorer\n",
    "import joblib\n",
    "\n",
    "from src.file_utils import find_kaggle_csv\n",
    "from src.preprocess import add_dateparts\n",
    "\n",
    "MODELS_DIR = Path(\"models\"); MODELS_DIR.mkdir(exist_ok=True)\n",
    "REPORTS_DIR = Path(\"reports\"); REPORTS_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca5ee9",
   "metadata": {},
   "source": [
    "## 1) Data ingestion + date parsing + sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = find_kaggle_csv(\"TrainAndValid\")\n",
    "print(\"Using:\", train_path)\n",
    "\n",
    "df = pd.read_csv(train_path, low_memory=False, parse_dates=[\"saledate\"])\n",
    "df = df.sort_values(\"saledate\").reset_index(drop=True)\n",
    "\n",
    "df.shape, df[\"saledate\"].min(), df[\"saledate\"].max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc27a7f",
   "metadata": {},
   "source": [
    "## 2) Time-series split (no random shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0952b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = pd.to_datetime(\"2012-01-01\")\n",
    "train_df = df[df[\"saledate\"] < split_date].copy()\n",
    "valid_df = df[df[\"saledate\"] >= split_date].copy()\n",
    "train_df.shape, valid_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7febd7f8",
   "metadata": {},
   "source": [
    "## 3) Feature engineering from `saledate` (drop raw datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a02da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_dateparts(train_df, \"saledate\", drop=True)\n",
    "valid_df = add_dateparts(valid_df, \"saledate\", drop=True)\n",
    "\n",
    "assert \"saledate\" not in train_df.columns\n",
    "assert \"saledate\" not in valid_df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087cc1c2",
   "metadata": {},
   "source": [
    "## 4) Initial inspection (dtypes, missing values, unique values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train FE shape:\", train_df.shape)\n",
    "display(train_df.dtypes)\n",
    "\n",
    "missing = train_df.isna().sum().sort_values(ascending=False)\n",
    "display(missing.head(25))\n",
    "print(\"Columns with missing:\", int((missing > 0).sum()))\n",
    "\n",
    "cols_to_check = [\"SalePrice\", \"YearMade\", \"MachineID\", \"ModelID\", \"state\", \"ProductSize\", \"UsageBand\"]\n",
    "cols_to_check = [c for c in cols_to_check if c in train_df.columns]\n",
    "for c in cols_to_check:\n",
    "    print(f\"\\n--- {c} ---\")\n",
    "    print(\"dtype:\", train_df[c].dtype, \"nunique:\", train_df[c].nunique(dropna=False))\n",
    "    display(train_df[c].value_counts(dropna=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5e73f",
   "metadata": {},
   "source": [
    "## 5) Build X/y and preprocess (categories, missing values, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba73c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"SalePrice\"\n",
    "\n",
    "X_train = train_df.drop(columns=[target])\n",
    "y_train = train_df[target].values\n",
    "X_valid = valid_df.drop(columns=[target])\n",
    "y_valid = valid_df[target].values\n",
    "\n",
    "for c in X_train.columns:\n",
    "    if X_train[c].dtype == \"object\":\n",
    "        X_train[c] = X_train[c].astype(\"category\")\n",
    "for c in X_valid.columns:\n",
    "    if X_valid[c].dtype == \"object\":\n",
    "        X_valid[c] = X_valid[c].astype(\"category\")\n",
    "\n",
    "dt_cols = list(X_train.select_dtypes(include=[\"datetime64[ns]\", \"datetime64\"]).columns)\n",
    "print(\"Datetime cols in X_train:\", dt_cols)\n",
    "if dt_cols:\n",
    "    X_train = X_train.drop(columns=dt_cols)\n",
    "    X_valid = X_valid.drop(columns=dt_cols, errors=\"ignore\")\n",
    "\n",
    "X_valid = X_valid.reindex(columns=X_train.columns)\n",
    "\n",
    "cat_cols = [c for c in X_train.columns if str(X_train[c].dtype) in (\"category\", \"object\")]\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "len(num_cols), len(cat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0146c35d",
   "metadata": {},
   "source": [
    "## 6) Modeling + evaluation (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "categorical = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", numeric, num_cols),\n",
    "    (\"cat\", categorical, cat_cols),\n",
    "], remainder=\"drop\", sparse_threshold=0.0)\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    max_features=\"sqrt\",\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"preprocess\", preprocess), (\"model\", model)])\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    y_pred_clip = np.maximum(0, y_pred)\n",
    "    return {\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "        \"rmsle\": float(np.sqrt(mean_squared_log_error(y_true, np.maximum(0, y_pred_clip)))),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_valid)\n",
    "baseline_metrics = evaluate(y_valid, pred)\n",
    "baseline_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f33aaab",
   "metadata": {},
   "source": [
    "## 7) Hyperparameter tuning (RandomizedSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_rmsle(y_true, y_pred):\n",
    "    y_pred = np.maximum(0, y_pred)\n",
    "    return -float(np.sqrt(mean_squared_log_error(y_true, y_pred)))\n",
    "\n",
    "scorer = make_scorer(neg_rmsle, greater_is_better=True)\n",
    "\n",
    "param_dist = {\n",
    "    \"model__n_estimators\": [200, 400, 700],\n",
    "    \"model__max_depth\": [None, 10, 20, 30],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\", 0.5],\n",
    "    \"model__min_samples_split\": [2, 5, 10],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring=scorer,\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "best_params = search.best_params_\n",
    "\n",
    "tuned_pred = best_model.predict(X_valid)\n",
    "tuned_metrics = evaluate(y_valid, tuned_pred)\n",
    "\n",
    "baseline_metrics, tuned_metrics, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae388d7f",
   "metadata": {},
   "source": [
    "## 8) Save model + metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a55d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_model, MODELS_DIR/\"model.joblib\")\n",
    "\n",
    "metrics_out = {\n",
    "    \"baseline\": baseline_metrics,\n",
    "    \"tuned\": tuned_metrics,\n",
    "    \"best_params\": best_params,\n",
    "    \"split_date\": \"2012-01-01\",\n",
    "    \"train_path_used\": str(train_path),\n",
    "}\n",
    "(MODELS_DIR/\"metrics.json\").write_text(json.dumps(metrics_out, indent=2))\n",
    "\n",
    "metrics_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66806b",
   "metadata": {},
   "source": [
    "## 9) Test predictions + custom prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = find_kaggle_csv(\"Test\")\n",
    "print(\"Using:\", test_path)\n",
    "\n",
    "test_df = pd.read_csv(test_path, low_memory=False, parse_dates=[\"saledate\"])\n",
    "test_df = test_df.sort_values(\"saledate\").reset_index(drop=True)\n",
    "test_df = add_dateparts(test_df, \"saledate\", drop=True)\n",
    "\n",
    "for c in test_df.columns:\n",
    "    if test_df[c].dtype == \"object\":\n",
    "        test_df[c] = test_df[c].astype(\"category\")\n",
    "\n",
    "test_pred = best_model.predict(test_df)\n",
    "test_pred[:5], len(test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\n",
    "    \"saledate\": \"2012-11-19\",\n",
    "    \"MachineID\": 999999,\n",
    "    \"ModelID\": 4605,\n",
    "    \"fiBaseModel\": 555,\n",
    "    \"YearMade\": 2005,\n",
    "    \"ProductSize\": \"Medium\",\n",
    "    \"state\": \"Florida\",\n",
    "    \"UsageBand\": \"High\",\n",
    "}\n",
    "sample_df = pd.DataFrame([sample])\n",
    "sample_df = add_dateparts(sample_df, \"saledate\", drop=True)\n",
    "for c in sample_df.columns:\n",
    "    if sample_df[c].dtype == \"object\":\n",
    "        sample_df[c] = sample_df[c].astype(\"category\")\n",
    "\n",
    "float(best_model.predict(sample_df)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3476faf0",
   "metadata": {},
   "source": [
    "## 10) Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = best_model.named_steps[\"model\"]\n",
    "importances = rf.feature_importances_\n",
    "feature_names = num_cols + cat_cols\n",
    "\n",
    "fi = pd.DataFrame({\"feature\": feature_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    "fi.to_csv(REPORTS_DIR/\"feature_importance.csv\", index=False)\n",
    "\n",
    "top = fi.head(20)[::-1]\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.barh(top[\"feature\"], top[\"importance\"])\n",
    "plt.title(\"Top 20 Feature Importances (RandomForest)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR/\"feature_importance_top20.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "fi.head(10)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
